{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuyKDfIjtWlK",
        "outputId": "e592a78d-9319-43fc-c4d3-6b3c0499cd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.16)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax) (1.11.3)\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.10-py3-none-any.whl (360 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (1.4.0)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (1.23.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku) (0.9.0)\n",
            "Installing collected packages: jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.10 jmp-0.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install jax\n",
        "!pip install dm-haiku"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax"
      ],
      "metadata": {
        "id": "-IfJd_kqkw4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devices = jax.devices()\n",
        "if len(devices) > 0:\n",
        "    jax.devices()[0]  # Use the first available GPU\n"
      ],
      "metadata": {
        "id": "9rGPVnhndj-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_bayes_linear(input_dim, output_dim, prior, rng):\n",
        "    weight_mus = jax.random.uniform(rng, (input_dim, output_dim), minval=-0.05, maxval=0.05)\n",
        "    weight_rhos = jax.random.uniform(rng, (input_dim, output_dim), minval=-2.0, maxval=-1.0)\n",
        "    bias_mus = jax.random.uniform(rng, (output_dim,), minval=-0.05, maxval=0.05)\n",
        "    bias_rhos = jax.random.uniform(rng, (output_dim,), minval=-2.0, maxval=-1.0)\n",
        "    return {\n",
        "        'weight_mus': weight_mus,\n",
        "        'weight_rhos': weight_rhos,\n",
        "        'bias_mus': bias_mus,\n",
        "        'bias_rhos': bias_rhos,\n",
        "        'prior': prior,\n",
        "    }\n",
        "\n",
        "def bayes_linear_normalq(params, x, sample=True):\n",
        "    weight_mus = params['weight_mus']\n",
        "    weight_rhos = params['weight_rhos']\n",
        "    bias_mus = params['bias_mus']\n",
        "    bias_rhos = params['bias_rhos']\n",
        "    prior = params['prior']\n",
        "\n",
        "    if sample:\n",
        "        # Sample gaussian noise for each weight and each bias\n",
        "        weight_epsilons = jax.random.normal(jax.random.PRNGKey(0), weight_mus.shape)\n",
        "        bias_epsilons = jax.random.normal(jax.random.PRNGKey(1), bias_mus.shape)\n",
        "\n",
        "        # Calculate the weight and bias stds from the rho parameters\n",
        "        weight_stds = jnp.log(1 + jnp.exp(weight_rhos))\n",
        "        bias_stds = jnp.log(1 + jnp.exp(bias_rhos))\n",
        "\n",
        "        # Calculate samples from the posterior from the sampled noise and mus/stds\n",
        "        weight_sample = weight_mus + weight_epsilons * weight_stds\n",
        "        bias_sample = bias_mus + bias_epsilons * bias_stds\n",
        "\n",
        "        output = jnp.dot(x, weight_sample) + bias_sample\n",
        "\n",
        "        # Computing the KL loss term\n",
        "        prior_cov, varpost_cov = prior['sigma'] ** 2, weight_stds ** 2\n",
        "        KL_loss = 0.5 * (jnp.log(prior_cov / varpost_cov)).sum() - 0.5 * weight_stds.size\n",
        "        KL_loss = KL_loss + 0.5 * (varpost_cov / prior_cov).sum()\n",
        "        KL_loss = KL_loss + 0.5 * ((weight_mus - prior['mu']) ** 2 / prior_cov).sum()\n",
        "\n",
        "        prior_cov, varpost_cov = prior['sigma'] ** 2, bias_stds ** 2\n",
        "        KL_loss = KL_loss + 0.5 * (jnp.log(prior_cov / varpost_cov)).sum() - 0.5 * bias_stds.size\n",
        "        KL_loss = KL_loss + 0.5 * (varpost_cov / prior_cov).sum()\n",
        "        KL_loss = KL_loss + 0.5 * ((bias_mus - prior['mu']) ** 2 / prior_cov).sum()\n",
        "\n",
        "        return output, KL_loss\n",
        "    else:\n",
        "        output = jnp.dot(x, weight_mus) + bias_mus\n",
        "        KL_loss = 0.0  # Inference without sampling, so KL loss is zero\n",
        "        return output, KL_loss\n"
      ],
      "metadata": {
        "id": "RjLH6bBvuDn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior = {'mu': 0.0, 'sigma': 0.1}\n",
        "rng = jax.random.PRNGKey(0)"
      ],
      "metadata": {
        "id": "tAqHtlRAOq2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the forward pass for the hidden layer\n",
        "def hidden_layer(x, params, sample=True):\n",
        "    output, kl_loss = bayes_linear_normalq(params, x, sample=sample)\n",
        "    return jax.nn.tanh(output), kl_loss\n",
        "\n",
        "# Define a neural network with 1 hidden layer\n",
        "def neural_network(x, params, sample=True):\n",
        "    # Hidden layer\n",
        "    hidden_output, hidden_kl_loss = hidden_layer(x, params['hidden_layer'], sample=sample)\n",
        "\n",
        "    # Output layer\n",
        "    output, output_kl_loss = bayes_linear_normalq(params['output_layer'], hidden_output, sample=sample)\n",
        "\n",
        "    # Combine KL losses\n",
        "    kl_loss = hidden_kl_loss + output_kl_loss\n",
        "\n",
        "    return output, kl_loss\n"
      ],
      "metadata": {
        "id": "x8po8BhWvzGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the parameters for the hidden layer\n",
        "hidden_layer_params = init_bayes_linear(input_dim=1, output_dim=64, prior=prior, rng=rng)"
      ],
      "metadata": {
        "id": "24qpUamx191X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Generate evenly spaced x values\n",
        "x = np.linspace(0, 1, 51).reshape(-1, 1)\n",
        "\n",
        "# Calculate y values\n",
        "y = np.sin(x * 2 * np.pi) + np.random.normal(size=(51, 1), scale=0.1)\n",
        "\n",
        "# Split the dataset into a training and evaluation seta\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(x, y, test_size=0.5, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "Iz8Hu04e2euc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'hidden_layer': hidden_layer_params,\n",
        "    'output_layer': init_bayes_linear(input_dim=64, output_dim=1, prior=prior, rng=rng),\n",
        "    'log_noise' : jnp.array([0.0])\n",
        "}\n",
        "output, kl_loss = neural_network(x, params, sample=True)"
      ],
      "metadata": {
        "id": "gjsUb80Y4R82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5 * jnp.square(target - output) / jnp.square(sigma)\n",
        "    log_coeff = -no_dim * jnp.log(sigma)\n",
        "\n",
        "    return -jnp.sum(log_coeff + exponent)\n"
      ],
      "metadata": {
        "id": "NZVvCXVtHCpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(params, x, y, rng):\n",
        "  fit_loss = 0.0\n",
        "  kl_loss = 0.0\n",
        "  for i in range(10):\n",
        "    predictions, kl_loss = neural_network(x, params, sample=True)\n",
        "    gaussian_loss = log_gaussian_loss(predictions, y, jnp.exp(params['log_noise']), 1)\n",
        "    fit_loss += gaussian_loss\n",
        "  total_loss = (fit_loss + kl_loss)/(10 * x.shape[0])\n",
        "\n",
        "  return total_loss, fit_loss, kl_loss\n",
        "\n",
        "\n",
        "def elbo_loss(params, x, y, rng):\n",
        "  return loss(params, x, y, rng)[0]"
      ],
      "metadata": {
        "id": "YCwBlsk_1vnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the optimizer\n",
        "opt = optax.sgd(learning_rate=1e-1)\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "# Training loop\n",
        "beta = 1\n",
        "num_steps = 500"
      ],
      "metadata": {
        "id": "YQ2TyE8i2gof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = params\n",
        "best_loss = 1000"
      ],
      "metadata": {
        "id": "ZOvZ-xJNlNEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(num_steps):\n",
        "    grads = jax.grad(elbo_loss)(params, X_train, y_train, rng)\n",
        "    # Update the parameters\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    total_loss, fit_loss, kl_loss = loss(params, X_train, y_train, rng)\n",
        "\n",
        "    if (total_loss < best_loss):\n",
        "      best_loss = total_loss\n",
        "      best_params = params\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}, Loss: {total_loss:.4f}, Fit Loss: {fit_loss:.4f}, KL Loss: {kl_loss:.4f}, noise: { jnp.exp(params['log_noise'])[0]:.4f}\")\n",
        "        # Print evaluation loss if needed\n",
        "\n",
        "# Continue with plotting the predictions and uncertainties.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZVMWYkD2mcX",
        "outputId": "50a267f5-781e-4845-aac7-a2977c02a7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 1.9046, Fit Loss: 172.4726, KL Loss: 303.6797, noise: 1.3111\n",
            "Step 10, Loss: 1.5681, Fit Loss: 125.5584, KL Loss: 266.4687, noise: 1.0851\n",
            "Step 20, Loss: 1.1971, Fit Loss: 94.9912, KL Loss: 204.2810, noise: 0.9425\n",
            "Step 30, Loss: 0.8637, Fit Loss: 72.8573, KL Loss: 143.0558, noise: 0.8477\n",
            "Step 40, Loss: 0.3254, Fit Loss: 58.2337, KL Loss: 23.1229, noise: 0.7879\n",
            "Step 50, Loss: 0.2975, Fit Loss: 50.1466, KL Loss: 24.2182, noise: 0.7535\n",
            "Step 60, Loss: 0.2833, Fit Loss: 44.8599, KL Loss: 25.9667, noise: 0.7334\n",
            "Step 70, Loss: 0.2747, Fit Loss: 41.2888, KL Loss: 27.3760, noise: 0.7205\n",
            "Step 80, Loss: 0.2688, Fit Loss: 38.7290, KL Loss: 28.4617, noise: 0.7118\n",
            "Step 90, Loss: 0.2640, Fit Loss: 36.7094, KL Loss: 29.2813, noise: 0.7053\n",
            "Step 100, Loss: 0.2591, Fit Loss: 34.8804, KL Loss: 29.9048, noise: 0.6999\n",
            "Step 110, Loss: 0.2533, Fit Loss: 32.9304, KL Loss: 30.4067, noise: 0.6947\n",
            "Step 120, Loss: 0.2455, Fit Loss: 30.5155, KL Loss: 30.8632, noise: 0.6887\n",
            "Step 130, Loss: 0.2342, Fit Loss: 27.1938, KL Loss: 31.3561, noise: 0.6809\n",
            "Step 140, Loss: 0.2173, Fit Loss: 22.3554, KL Loss: 31.9794, noise: 0.6699\n",
            "Step 150, Loss: 0.1921, Fit Loss: 15.1698, KL Loss: 32.8456, noise: 0.6541\n",
            "Step 160, Loss: 0.1549, Fit Loss: 4.6438, KL Loss: 34.0798, noise: 0.6314\n",
            "Step 170, Loss: 0.1035, Fit Loss: -9.9060, KL Loss: 35.7727, noise: 0.6004\n",
            "Step 180, Loss: 0.0418, Fit Loss: -27.3782, KL Loss: 37.8326, noise: 0.5628\n",
            "Step 190, Loss: -0.0134, Fit Loss: -43.0856, KL Loss: 39.7400, noise: 0.5262\n",
            "Step 200, Loss: -0.0453, Fit Loss: -51.9971, KL Loss: 40.6683, noise: 0.5010\n",
            "Step 210, Loss: -0.0593, Fit Loss: -55.1710, KL Loss: 40.3393, noise: 0.4895\n",
            "Step 220, Loss: 0.0338, Fit Loss: -30.8525, KL Loss: 39.3105, noise: 0.4897\n",
            "Step 230, Loss: -0.0390, Fit Loss: -47.1982, KL Loss: 37.4605, noise: 0.5825\n",
            "Step 240, Loss: -0.0749, Fit Loss: -55.4490, KL Loss: 36.7150, noise: 0.4982\n",
            "Step 250, Loss: -0.0126, Fit Loss: -39.0086, KL Loss: 35.8591, noise: 0.4896\n",
            "Step 260, Loss: -0.0603, Fit Loss: -49.6952, KL Loss: 34.6287, noise: 0.5661\n",
            "Step 270, Loss: -0.0862, Fit Loss: -55.4883, KL Loss: 33.9494, noise: 0.4953\n",
            "Step 280, Loss: 0.0566, Fit Loss: -19.5602, KL Loss: 33.7055, noise: 0.6260\n",
            "Step 290, Loss: -0.0895, Fit Loss: -54.8714, KL Loss: 32.5076, noise: 0.5066\n",
            "Step 300, Loss: 0.2084, Fit Loss: 19.2337, KL Loss: 32.8625, noise: 0.5848\n",
            "Step 310, Loss: -0.0881, Fit Loss: -53.4074, KL Loss: 31.3750, noise: 0.5091\n",
            "Step 320, Loss: 0.0504, Fit Loss: -18.8913, KL Loss: 31.4794, noise: 0.5789\n",
            "Step 330, Loss: -0.0688, Fit Loss: -47.6788, KL Loss: 30.4790, noise: 0.5068\n",
            "Step 340, Loss: -0.0496, Fit Loss: -42.6409, KL Loss: 30.2490, noise: 0.5553\n",
            "Step 350, Loss: 0.0136, Fit Loss: -26.5055, KL Loss: 29.9013, noise: 0.5139\n",
            "Step 360, Loss: -0.0723, Fit Loss: -47.4476, KL Loss: 29.3723, noise: 0.5296\n",
            "Step 370, Loss: 0.0588, Fit Loss: -14.7751, KL Loss: 29.4676, noise: 0.5401\n",
            "Step 380, Loss: -0.0545, Fit Loss: -42.3541, KL Loss: 28.7388, noise: 0.5176\n",
            "Step 390, Loss: -0.0317, Fit Loss: -36.5357, KL Loss: 28.6199, noise: 0.5419\n",
            "Step 400, Loss: 0.0019, Fit Loss: -27.8629, KL Loss: 28.3503, noise: 0.5234\n",
            "Step 410, Loss: -0.0550, Fit Loss: -41.6885, KL Loss: 27.9491, noise: 0.5275\n",
            "Step 420, Loss: -0.0007, Fit Loss: -28.0862, KL Loss: 27.9143, noise: 0.5363\n",
            "Step 430, Loss: -0.0335, Fit Loss: -35.8936, KL Loss: 27.5159, noise: 0.5230\n",
            "Step 440, Loss: -0.0395, Fit Loss: -37.2205, KL Loss: 27.3340, noise: 0.5331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of samples to draw from the posterior\n",
        "num_samples = 100\n",
        "\n",
        "# Lists to store predicted values and uncertainties\n",
        "predicted_values = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    sampled_predictions, _ = neural_network(x, best_params, sample=True)\n",
        "    predicted_values.append(sampled_predictions)\n",
        "\n",
        "# Calculate the mean and standard deviation of the predictions\n",
        "mean_predictions = jnp.mean(jnp.stack(predicted_values), axis=0)\n",
        "std_predictions = jnp.std(jnp.stack(predicted_values), axis=0)\n",
        "uncertainity = (best_params['log_noise'] ** 2 + std_predictions ** 2) ** 0.5"
      ],
      "metadata": {
        "id": "jbQnNeSX2pv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort the data for cleaner plotting\n",
        "sorted_indices = np.argsort(x.flatten())\n",
        "x_sorted = x[sorted_indices]\n",
        "y_sorted = y[sorted_indices]\n",
        "mean_predictions_sorted = mean_predictions[sorted_indices]\n",
        "std_predictions_sorted = std_predictions[sorted_indices]\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = plt.gca()\n",
        "\n",
        "# Plot the true data\n",
        "ax.scatter(x_sorted, y_sorted, label='True Data', marker=\"o\", alpha=0.7, lw=0.6, color='black')\n",
        "\n",
        "# Plot the mean predictions\n",
        "ax.plot(x_sorted, mean_predictions_sorted, label='Mean Predictions', color='blue')\n",
        "\n",
        "# Fill between one standard deviation of the predictions (1 std)\n",
        "std_plt = plt.gca().fill_between(\n",
        "        x_sorted.squeeze(), (mean_predictions_sorted - 1 * uncertainity).squeeze(), (mean_predictions_sorted + 1 * uncertainity).squeeze(),\n",
        "        color='lightgray', alpha=0.5 / 1, label=f'Uncertainty ({1} std dev)')\n",
        "\n",
        "\n",
        "# Set axis labels and title\n",
        "ax.set_xlabel('X Data')\n",
        "ax.set_ylabel('Y Data')\n",
        "ax.set_title('Bayesian Neural Network Prediction Plot')\n",
        "\n",
        "# Add a legend and grid\n",
        "ax.legend()\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qDG2RJB53THS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoI4wM9eDXKF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}